{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "It is a study of computer algorithm that improve automatically through experience. These algorithms build a Mathematical model (Neural Network) that is based on a sample called the \"training data\".\n",
    "\n",
    "# What are Neural Networks?\n",
    "\n",
    "It is a series of algorithms built such that they find the patterns and relations in a set of data, that mimics the working of the human brain.\n",
    "\n",
    "<img src=\"assets/NeuralNetwork.png\" width=600px>\n",
    "\n",
    "Each of the nodes in the neural network is called a perceptron.\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "Every unit of a Neural Network is called a perceptron. It is essentially an artificial neuron.\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Each perceptron has some number of weighted inputs and a bias, which are summed up (linear combination) and then passed through an activation function to get the output.\n",
    "\n",
    "Mathematically this looks like: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With vectors this is the dot/inner product of two vectors:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "They are mathematical equations used to determine the output of a neural network.\n",
    "\n",
    "<img src=\"assets/Activation_Functions.png\" width=700px>\n",
    "\n",
    "\n",
    "\n",
    "## Tensors\n",
    "\n",
    "Neural network computation is essentially just a bunch of linear algebra operations on *tensors*. Tensors are the data structures used for Neural Network computation and PyTorch and other such Deep Learning Libararies are built around them.\n",
    "\n",
    "<img src=\"assets/Tensors.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Perceptron \n",
    "\n",
    "Now let's create our first perceptron/neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pytorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define activation function (sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do multiplication and sum in the same operation using matrix multiplication. Matrix multiplication is much more efficient with modern libraries and GPUs.\n",
    "\n",
    "Here we'll use [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) to perform matrix multiplication on features and weights.\n",
    "\n",
    "**Note** : For matrix multiplication we need to have the two matrix in the form [M x N] and [N x O] .\n",
    "\n",
    "Here in our case we have both the Tensors in a vector form which is [1 x 5].\n",
    "\n",
    "To counter this issue we need to reshape this Tensor. We have a few options [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "\n",
    "* `weights.reshape(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
    "* `weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n",
    "\n",
    "> In today's session we'll use `.view()` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using torch.mm()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our first neural network\n",
    "\n",
    "Well that was a single neuron, which alone doesn't do much but when stacked together they showcase their real potential.\n",
    "\n",
    "<img src=\"assets/multilayer_diagram_weights.png\" width=500px>\n",
    "\n",
    "Here the bottom layer provides the inputs and is called the **Input layer**. The layer that follows is what is called as the **Hidden layer**. The topmost layer, provides us with the output and is fittingly called the **Output layer**.\n",
    "\n",
    "This network can be represented mathematically with matrices as shown below.\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "\n",
    "\n",
    "# Features \n",
    "\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "\n",
    "\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "\n",
    "\n",
    "# Weights for hidden layer to output layer\n",
    "\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once executed correctly you should see the output `tensor([[ 0.3171]])`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
